{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN networks\n",
    "\n",
    "This notebook is for RNN related reflection questions in TT00BX20 Luonnollisen kielen käsittely -course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "1. What is RNN? What about LSTM or GRU?\n",
    "2. When you should initialize RNN hidden layer? And how you should initialize layer?\n",
    "3. What is attention method?\n",
    "4. What is generative network?\n",
    "5. What are Start-of-Sentence and End-of-Sentence markers?\n",
    "6. What is Bleu score?\n",
    "7. What are transformer networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **RNN** tulee sanoista Recurrent Neural Network, joka tarkoittaa takaisin kytkettyä verkkoa, mikä taas tarkoittaa sitä, että verkossa on kytkentöjä myös taaksepäin, jonka ansioista verkolla on muisti, toisin kuin esimerkiksi konvoluutio- ja FC-verkoilla. RNN-verkkoja käytetään mm. luonnollisen kielen käsittelyssä juuri tämän ominaisuuden takia, jotta he muistasivat esimerkiksi asianyhteyden paremmin. **LSTM** on muistin sisältävä neuroverkkoarkkitehtuuri, jolla on kytkentöjä taaksepäin. LSTM tulee sanoista Long-Short Term Memory. LSTM-verkot soveltuvat hyvin luokitteluun ja ennustamiseen. **GRU** on myös eräs RNN-arkkitehtuuri, joka tulee sanoista Gated Recurrent Unit. GRU on näistä kolmesta mallista uusin arkkitehtuuri. GRU-verkko on normaalia LSTM-verkkoa yksinkertaisempi, jonka vuoksi GRU-verkko on kasvanut suositummaksi viime vuosien aikana.\n",
    "\n",
    "2. Piilokerrosten alustus täytyy tehdä ennen verkon käyttöä. \n",
    "3. Attention metodi voidaan suomentaa vaikka tarkkailutoiminnoksi. Se parantaa neuroverkon suoriutumiskykyä ja oppimista, sillä sen avulla decoder oppii tietylle syötesekvenssin alueelle. Ensin lasketaan attention painokertoimet ja nämä kerrotaan encoderin ulostulolla. Tulos auttaa decoderia valitsemaan oikeat sanat outputtiin. Nämä attentionin painokertoimet lasketaan feed-forward layerilla käyttäen inputtina decorin inputtia ja edellistä hidden tilaa.\n",
    "4. Neuroverkkoarkkitehtuuri, jossa on ikäänkuin yhdistetty kaksi neuroverkkoa keskenään. Toisella verkolla on tarkoituksenaan luoda aidon oloista dataa ja toinen verkko taas pyrkii erottamaan neuroverkon luoman datan oikeasta datasta. Esimerkiksi NVIDIA on tällä tekniikalla luonut mallin, joka generoi täysin aidon näköisiä ihmiskasvoja, joita ei kuitenkaan ole oikeasti olemassa. \n",
    "5. Start-of-Sentence eli `<SOS>` ja End-of-Sentence eli `<EOS>` erottaa lauseen alun ja lopun.\n",
    "6. Bleu score on  metodi, jolla mitataan neuroverkon tekemän käännöksen hyvyyttä/tarkkuutta. Bleu scoressa verrataan verkon antamaa käännöstä oikeaan käännökseen sen perusteella, kuinka paljon lauseet sisältävät samoja sanoja.\n",
    "7. Transformer on neuroverkkomalli, jota käytetään päästööntöisesti luonnollisen kielen käsittelyssä. Se on melko uutta teknologiaa ja sen arkkitehtuurissa hyödynnetään mm. aiemmin mainittua tarkkailutoimintoa (attention method) ja sen liäksi myös decoderia ja encoderia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
