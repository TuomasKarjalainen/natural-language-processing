{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Processing Pipelines\n",
    "\n",
    "This chapter will show you everything you need to know about spaCy's processing pipeline. You'll learn what goes on under the hood when you process a text, how to write your own components and add them to the pipeline, and how to use custom attributes to add your own metadata to the documents, spans and tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "### 3.1 Inspecting the pipeline\n",
    "\n",
    "Let’s inspect the small English model’s pipeline!\n",
    "\n",
    "- Load the `en_core_web_sm` model and create the `nlp` object.\n",
    "- Print the names of the pipeline components using `nlp.pipe_names`.\n",
    "- Print the full pipeline of `(name, component)`\n",
    " tuples using `nlp.pipeline`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f6c2ec9bb48>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f6c2ec9b620>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f6c2ee0a800>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f6c2ee0a798>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f6c2ec1e848>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f6c2ec1ef48>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "# small pipeline paketti\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1 What happens when you call nlp?\n",
    "\n",
    "What does spaCy do when you call nlp on a string of text?\n",
    "\n",
    "```doc = nlp(\"This is a sentence.\")````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here...*\n",
    "\n",
    "- `nlp` tokenoi tekstin ja tekee siitä dokumentin (`doc`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2 Use cases for custom components\n",
    "\n",
    "Which of these problems can be solved by custom pipeline components? Choose all that apply!\n",
    "\n",
    "1. Updating the pre-trained models and improving their predictions\n",
    "2. Computing your own values based on tokens and their attributes\n",
    "3. Adding named entities, for example based on a dictionary\n",
    "4. Implementing support for an additional language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here...*\n",
    "\n",
    "Väärät:\n",
    "- **Computing your own values based on tokens and their attributes**\n",
    "- **Adding named entities, for example based on a dictionary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Simple components\n",
    "\n",
    "The example shows a custom component that prints the number of tokens in a document. Can you complete it?\n",
    "\n",
    "- Complete the component function with the doc’s length.\n",
    "- Add the length_component to the existing pipeline as the first component.\n",
    "- Try out the new pipeline and process any text with the nlp object – for example “This is a sentence.”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "This document is 5 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "\n",
    "@Language.component(\"length_component\")\n",
    "def component_func(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Complex components\n",
    "\n",
    "In this exercise, you’ll be writing a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the `doc.ents`. A `PhraseMatcher` with the animal patterns has already been created as the variable `matcher`.\n",
    "\n",
    "- Define the custom component and apply the `matcher` to the `doc`.\n",
    "- Create a Span for each match, assign the label ID for `\"ANIMAL\"` and overwrite the `doc.ents` with the new spans.\n",
    "- Add the new component to the pipeline after the `\"ner\"` component.\n",
    "- Process the text and print the entity text and entity label for the entities in `doc.ents`.\n",
    "\n",
    "\n",
    "#### Exercise 3.3 Following code is written for spaCy v2.0 update code for spaCy v3.0\n",
    "Hint. look code above as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function animal_component at 0x7f6bea051378> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6852f1116097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Add the component to the pipeline after the \"ner\" component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manimal_component\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function animal_component at 0x7f6bea051378> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(animal_component, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'animal_component', 'attribute_ruler', 'lemmatizer']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# --------------\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension attributes\n",
    "\n",
    "### 3.4 Setting extension attributes (1)\n",
    "\n",
    "Let’s practice setting some extension attributes.\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "- Use Token.set_extension to register \"is_country\" (default False).\n",
    "- Update it for \"Spain\" and print it for all tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "Use Token.set_extension to register \"reversed\" (getter function get_reversed).\n",
    "Print its value for each token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed, force=True)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Setting extension attributes (2)\n",
    "\n",
    "Let’s try setting some more complex attributes using getters and method extensions.\n",
    "\n",
    "#### Exercise 3.4: Complete the code:\n",
    "\n",
    "- Complete the get_has_number function .\n",
    "- Use Doc.set_extension to register \"has_number\" (getter get_has_number) and print its value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n",
      "has_number: False\n",
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Exercise 3.4: Complete the code:\n",
    "\n",
    "# 1. Define 'get_has_number(doc)' function, that returns true if any token in the 'doc' is 'like_num' \n",
    "# Hint. see code above for reference\n",
    "def get_has_number(doc):\n",
    "    # any = jos yksikään tokeneista saa arvon True\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# 2. Register the Doc property extension 'has_number' with the getter 'get_has_number'\n",
    "# Hint. see code above for reference\n",
    "\n",
    "Doc.set_extension('has_number', getter=get_has_number, force=True)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc1 = nlp(\"The museum is closed for five months this year.\")\n",
    "doc2 = nlp(\"The museum is closed.\")\n",
    "doc3 = nlp(\"The museum opened in 2012.\")\n",
    "print(\"has_number:\", doc1._.has_number)\n",
    "print(\"has_number:\", doc2._.has_number)\n",
    "print(\"has_number:\", doc3._.has_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hint.*** *You should get print*\n",
    "```\n",
    "has_number: True\n",
    "has_number: False\n",
    "has_number: True\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Entities and extensions\n",
    "\n",
    "In this exercise, you’ll combine custom extension attributes with the model’s predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location.\n",
    "\n",
    "- Complete the get_wikipedia_url getter so it only returns the URL if the span’s label is in the list of labels.\n",
    "- Set the Span extension \"wikipedia_url\" using the getter get_wikipedia_url.\n",
    "- Iterate over the entities in the doc and output their Wikipedia URL.\n",
    "\n",
    "\n",
    "#### Exercise 3.5: Add wikipedia search also for organizations and places (countries/cities/states):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie     PERSON     https://en.wikipedia.org/w/index.php?search=David_Bowie\n",
      "England         GPE        https://en.wikipedia.org/w/index.php?search=England\n",
      "CISCO           ORG        https://en.wikipedia.org/w/index.php?search=CISCO\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL \n",
    "    # Lisätään labelit organisaatioille, maille, kaupungeille ja osa-valtioille\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture. He lives in England.\" \n",
    "    \"But does he work for CISCO?\"\n",
    ")\n",
    "\n",
    "# Etsitään tekstistä kyseiset labelit\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in (\"PERSON\"):\n",
    "        print('%-15s' % ent.text, '%-10s' % ent.label_, '%-30s' %ent._.wikipedia_url)\n",
    "    elif ent.label_ in (\"ORG\"):\n",
    "        print('%-15s' % ent.text, '%-10s' % ent.label_, '%-30s' %ent._.wikipedia_url)\n",
    "    elif ent.label_ in (\"GPE\"):\n",
    "        print('%-15s' % ent.text, '%-10s' % ent.label_, '%-30s' %ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint.** You should get following print:\n",
    "\n",
    "```\n",
    "David Bowie     PERSON     https://en.wikipedia.org/w/index.php?search=David_Bowie\n",
    "England         GPE        https://en.wikipedia.org/w/index.php?search=England\n",
    "CISCO           ORG        https://en.wikipedia.org/w/index.php?search=CISCO\n",
    "```\n",
    "\n",
    "### 3.7 Components with extensions\n",
    "\n",
    "Extension attributes are especially powerful if they’re combined with custom pipeline components. In this exercise, you’ll write a pipeline component that finds country names and a custom extension attribute that returns a country’s capital, if available.\n",
    "\n",
    "A phrase matcher with all countries is available as the variable matcher. A dictionary of countries mapped to their capital cities is available as the variable CAPITALS.\n",
    "\n",
    "- Complete the countries_component and create a Span with the label \"GPE\" (geopolitical entity) for all matches.\n",
    "- Add the component to the pipeline.\n",
    "- Register the Span extension attribute \"capital\" with the getter get_capital.\n",
    "- Process the text and print the entity text, entity label and entity capital for each entity span in doc.ents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "Czech Republic GPE Prague\n",
      "Slovakia GPE Bratislava\n",
      "Switzerland GPE Bern\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.language import Language\n",
    "\n",
    "with open(\"data/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"data/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def component_func(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension('capital', getter=get_capital, force=True)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace. But who os protecting Switzerland air space?\")\n",
    "for ent in doc.ents :\n",
    "    print(ent.text, ent.label_, ent._.capital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint.** You should get print:\n",
    "```\n",
    "['countries_component']\n",
    "Czech Republic GPE Prague\n",
    "Slovakia GPE Bratislava\n",
    "Switzerland GPE Bern\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and performance\n",
    "### 3.10 Processing streams\n",
    "\n",
    "In this exercise, you’ll be using nlp.pipe for more efficient text processing. The nlp object has already been created for you. A list of tweets about a popular American fast food chain are available as the variable TEXTS.\n",
    "\n",
    "**Part 1**\n",
    "\n",
    "- Rewrite the example to use `nlp.pipe()`. Instead of iterating over the texts and processing them, iterate over the `doc` objects yielded by `nlp.pipe()`.\n",
    "- **Hint.** Using nlp.pipe lets you merge the first two lines of code into one.\n",
    "nlp.pipe takes the TEXTS and yields doc objects that you can loop over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['open']\n",
      "['terrible', 'payin']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"data/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for text in TEXTS:\n",
    "    doc = nlp(text)\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**\n",
    "\n",
    "- Rewrite the example to use nlp.pipe. Don’t forget to call list() around the result to turn it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['open']\n",
      "['terrible', 'payin']\n"
     ]
    }
   ],
   "source": [
    "for doc in list(nlp.pipe(TEXTS)):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) () () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"data/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = [nlp(text) for text in TEXTS]\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3**\n",
    "\n",
    "- Rewrite the example to use nlp.pipe. Don’t forget to call list() around the result to turn it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) () () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[David Bowie, Angela Merkel, Lady Gaga]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = [nlp(person) for person in people]\n",
    "\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[David Bowie, Angela Merkel, Lady Gaga]\n"
     ]
    }
   ],
   "source": [
    "patterns = list(nlp.pipe(people))\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data with context\n",
    "\n",
    "In this exercise, you’ll be using custom attributes to add author and book meta information to quotes.\n",
    "\n",
    "A list of [text, context] examples is available as the variable DATA. The texts are quotes from famous books, and the contexts dictionaries with the keys \"author\" and \"book\".\n",
    "\n",
    "- Use the set_extension method to register the custom attributes \"author\" and \"book\" on the Doc, which default to None.\n",
    "- Process the [text, context] pairs in DATA using nlp.pipe with as_tuples=True.\n",
    "- Overwrite the doc._.book and doc._.author with the respective info passed in as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\n",
      " — 'Metamorphosis' by Franz Kafka\n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing.\n",
      " — 'Moby-Dick or, The Whale' by Herman Melville\n",
      "\n",
      "It was the best of times, it was the worst of times.\n",
      " — 'A Tale of Two Cities' by Charles Dickens\n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\n",
      " — 'On the Road' by Jack Kerouac\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      " — '1984' by George Orwell\n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing.\n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"data/bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Doc extension \"author\" (default None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Register the Doc extension \"book\" (default None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(f\"{doc.text}\\n — '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Selective processing\n",
    "\n",
    "In this exercise, you’ll use the nlp.make_doc and nlp.disable_pipes methods to only run selected components when processing a text.\n",
    "\n",
    "**Part 1**\n",
    "\n",
    "- Rewrite the code to only tokenize the text using nlp.make_doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**\n",
    "\n",
    "- Disable the tagger and parser using the nlp.disable_pipes method.\n",
    "- Process the text and print all entities in the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-14 18:14:25,289] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Chick'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,290] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '-'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,290] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'fil'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,291] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '-'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,292] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'A'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,292] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,293] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'an'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,293] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'American'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,294] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'fast'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,295] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'food'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,296] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'restaurant'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,296] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'chain'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,297] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'headquartered'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,298] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,298] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,299] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'city'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,299] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'of'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,300] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'College'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,300] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Park'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,301] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,301] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Georgia'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,302] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,302] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'specializing'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,303] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,303] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'chicken'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,304] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'sandwiches'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-14 18:14:25,304] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "1. Is it possible to update spaCy models? How?\n",
    "2. Why you should be careful with training data?\n",
    "3. Miksi luonnollisen kielen käsitely on niin vaikeaa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Is it possible to update spaCy models? How?**\n",
    "\n",
    "On mahdollista päivittää, jotta saataisiin esimerkiksi parempia tuloksia luokittelutehtävissä tiettyihin ongelmiin. Alustetaan mallin painokertoimet randomisti ja tämän jälkeen kokeillaan tehdä muutama ennuste niillä. Verrataan tuttuun tapaan ennustetta oikeaan labeliin ja lasketaan tarkkuus, sekä miten päivittää kertoimia, jotta saataisiin parempia tuloksia. Ja toistetaan tätä mallin koulutusta.\n",
    "\n",
    "**2. Why you should be careful with training data?**\n",
    "\n",
    "Harjoitusdatan tarvitsee sisältää tekstiä, vähintäänkin lauseita, kappaleen tai vielä pidemmän dokumentin. Harjoitusdatassa täytyy olla esimerkkejä siitä, mitä me haluataan mallin ennustavan kontekstistä. Siinä pitää myös olla uusien esimerkkejen lisäksi vanhoja esimerkkejä.\n",
    "\n",
    "\n",
    "**3. Miksi luonnollisen kielen käsitely on niin vaikeaa?**\n",
    "\n",
    "Luonnollisen kielen käsitelyssä data-aineistona on tekstiä, mikä on ihmisen kirjoittamaa tai puhumaa. Datan käsittely on monivaiheista ja siinä voi tulla vastaan useita ongelmia, koska koneen on välillä vaikea ymmärtää ja hahmottaa kontekstistä asian yhteyttä, huumoria, murteita, kielioppia, sarkasmia, kirjoitusvirheitä ym. seikkoja. Toisekseen maailmassa on useita eri kieliä, joista toiset (suosituimmat) on paremmin tuettuja kuin toiset, eikä läheskään kaikille kielille edes löydy paketteja.      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
